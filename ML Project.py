# -*- coding: utf-8 -*-
"""Copy of ML  Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nt5UhtpClHt_7-82OKyjg_SLNGK_Thgc
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/Heart Attack project (1).csv')

df.head()

df.shape

df.info()

df.isna().sum()

df.duplicated().sum()

df.nunique()

df.columns

df.columns

"""**Preprocessing**"""

# Drop unnecessary columns
df.drop(['Patient ID','Hemisphere','Continent','Country','Physical Activity Days Per Week','Income'],axis=1,inplace=True)

df.head()

# Split and create column

df[["systolic","diastolic"]] = df['Blood Pressure'].str.split("/",expand = True)
df["systolic"] = pd.to_numeric(df["systolic"])
df['diastolic'] = pd.to_numeric(df["diastolic"])

df.info()

"""**Encoding**"""

from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder()

df['Sex'] = enc.fit_transform(df[['Sex']]).toarray()
df.head()

df['Diabetes'] = enc.fit_transform(df[['Diabetes']]).toarray()
df['Family History'] = enc.fit_transform(df[['Family History']]).toarray()
df['Smoking'] = enc.fit_transform(df[['Smoking']]).toarray()
df['Obesity'] = enc.fit_transform(df[['Obesity']]).toarray()
df['Alcohol Consumption'] = enc.fit_transform(df[['Alcohol Consumption']]).toarray()
df['Previous Heart Problems'] = enc.fit_transform(df[['Previous Heart Problems']]).toarray()
df['Medication Use'] = enc.fit_transform(df[['Medication Use']]).toarray()

df.sample(5)

df["Diet"].unique()

from sklearn.preprocessing import OrdinalEncoder
oe = OrdinalEncoder(categories = [['Healthy','Average','Unhealthy']])
oe

df['Diet'] = oe.fit_transform(df[['Diet']])

df['Diet'].head()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le

df["Heart Attack Risk"] = le.fit_transform(df[["Heart Attack Risk"]])
df.head()

"""**Resample**"""

df['Heart Attack Risk'].value_counts()

minority  = df[df["Heart Attack Risk"] == 1]
majority = df[df["Heart Attack Risk"] == 0]

majority.shape

minority.shape

from sklearn.utils import resample

upsampling = resample(minority,n_samples = len(majority),replace=True)

heart = pd.concat([majority,upsampling],axis = 0)

heart["Heart Attack Risk"].value_counts()

df.columns

"""**Test Train Split**"""

X = df.drop(['Heart Attack Risk','Blood Pressure'],axis = 1)
y = df['Heart Attack Risk']

X.head()

y.head()

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.13,random_state=42)

print(X_train.shape,X_test.shape)

X_train

X_test

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

X_train

X_test

"""**Modaling**"""

#  Regression machine learning libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

# Initializing the models
lr = LogisticRegression()
rf = RandomForestClassifier()
knn = KNeighborsClassifier()
svm = SVC()
dt = DecisionTreeClassifier()
gb = GradientBoostingClassifier()
xgb = XGBClassifier()
ab = AdaBoostClassifier()

# list of machine learning models
algorithm_obj = [lr,rf,knn,svm,dt,gb,xgb,ab]

# list of model names
algorithm_names = ["LogisticRegression" , "RandomForestClassifier" ,
                   "KNeighborsClassifier" , "SVR" , "DecisionTreeClassifier" , "GradientBoostingClassifier",
                   "XGBClassifier", "AdaBoostClassifier"  ]

# Iterate through each model and evaluate performance
for i,j in zip(algorithm_obj,algorithm_names):
  i.fit(X_train,y_train)
  train_pred = i.predict(X_train)
  test_pred = i.predict(X_test)

# prompt: how to find overfitting in numerical values

  print(j)
  print(" ")

# Calculate the difference between training and testing accuracy.

  print(f"Train Accurcy score : {accuracy_score(y_train,train_pred)}")
  print(f"test Accurcy score : {accuracy_score(y_test,test_pred)}")
  print(f"accuracy : {(accuracy_score(y_train,train_pred))-(accuracy_score(y_test,test_pred))}")
  print("-" *40)
  print(" "*40)

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the model
knn = KNeighborsClassifier()

# Define hyperparameter grid
param_grid = {
    'n_neighbors': range(1, 51),
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'p': [1, 2]
}

# Set up GridSearchCV
grid_search = GridSearchCV(knn, param_grid, cv=2, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
test_pred = grid_search.predict(X_test)
train_pred = grid_search.predict(X_train)

print("train accuracy: ",accuracy_score(y_train,train_pred))
print("test accuracy: ",accuracy_score(y_test,test_pred))
print("accuracy: ",accuracy_score(y_train,train_pred)-accuracy_score(y_test,test_pred))

from sklearn.model_selection import GridSearchCV

# Example: Tuning hyperparameters for a RandomForestClassifier
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Create the model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()

# Set up GridSearchCV
grid_search = GridSearchCV(model,param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
test_accuracy = grid_search.predict(X_test)
train_accuracy = grid_search.predict(X_train)
print(accuracy_score(y_test,test_accuracy))
print(accuracy_score(y_train,train_accuracy))
print(accuracy_score(y_test,test_accuracy)-accuracy_score(y_train,train_accuracy))

X.columns

Age = float(input("Enter the Age :"))
Sex = float(input("Enter the Sex :"))
Cholesterol = float(input("Enter the Cholesterol :"))
Heart_Rate = float(input("Enter the Heart Rate :"))
Diabetes = float(input("Enter the Diabetes :"))
Family_History = float(input("Enter the Family History :"))
Smoking = float(input("Enter the Smoking :"))
Obesity = float(input("Enter the Obesity :"))
Alcohol_Consumption = float(input("Enter the Alcohol Consumption :"))
Exercise_Hours_Per_Week = float(input("Enter the Exercise Hours Per Week :"))
Previous_Heart_Problems = float(input("Enter the Previous Heart Problems :"))
Medication_Use = float(input("Enter the Medication Use :"))
Stress_Level = float(input("Enter the Stress Level :"))
Sedentary_Hours_Per_Day = float(input("Enter the Sedentary Hours Per Day :"))
BMI = float(input("Enter the BMI :"))
Triglycerides = float(input("Enter the Triglycerides :"))
Sleep_Hours_Per_Day = float(input("Enter the Sleep Hours Per Day :"))
Heart_Attack_Risk = float(input("Enter the Heart Attack Risk :"))
systolic = float(input("Enter the systolic :"))
diastolic = float(input("Enter the diastolic :"))

# Use the correct variable names (with capital letters) in the input_data tuple
input_data = (Age, Sex, Cholesterol, Heart_Rate, Diabetes,
              Family_History, Smoking, Obesity, Alcohol_Consumption,
              Exercise_Hours_Per_Week, Previous_Heart_Problems,
              Medication_Use, Stress_Level, Sedentary_Hours_Per_Day,
              BMI, Triglycerides, Sleep_Hours_Per_Day, Heart_Attack_Risk,
              systolic, diastolic)

#change to array
input_data_arr = np.array( 67	,0.0,	208,	72	1.0	,1.0,	0.0,	1.0,	1.0,	4.168189,	1.0,	1.0,	1.0,	9,	6.615001,	31.251233,	286,	6,	158,	88
)

#reshape
reshape_array = input_data_arr.reshape(1,-1)

pred = lr.predict(reshape_array)
if any(input_data_arr) ==  0:
  print("not itdentifie")
else:
  print("-"*50)
  print(pred)

'''    67	0.0	208	72	1.0	1.0	0.0	1.0	1.0	4.168189	1.0	1.0	1.0	9	6.615001	31.251233	286	6	158	88  '''